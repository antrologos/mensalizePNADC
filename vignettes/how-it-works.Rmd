---
title: "How PNADCperiods Works"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{How PNADCperiods Works}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!--
MAINTAINER NOTE:
This vignette uses eval=FALSE for all code chunks.
Pre-computed figures and tables are generated by:
  code/generate_how_it_works_figures.R

Figures are stored in: vignettes/figures/how-it-works/
Tables are stored in: output/vignette/tables/how-it-works/

To regenerate, run the generation script. The script caches intermediate
results in data/processed/ for faster subsequent runs.
-->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

## Introduction

This vignette explains **how the PNADCperiods algorithm works**: the methodology behind converting Brazil's quarterly PNADC survey into sub-quarterly time series with calibrated weights. For practical usage examples, see the [Get Started](getting-started.html) vignette.

### What This Package Does

The `PNADCperiods` package transforms Brazil's quarterly PNADC (Pesquisa Nacional por Amostra de Domicilios Continua) survey data into finer-grained time series for labor market, demographic, and socioeconomic analysis. It addresses a fundamental challenge: PNADC reports quarterly aggregates, but policy analysis often requires understanding what happened within specific months, fortnights, or weeks.

**Package capabilities:**

1. **Multi-granularity period identification**: Determines which specific time period each observation refers to
   - **Monthly** (~97% determination rate) - suitable for most analyses
   - **Fortnight** (~3% determination rate) - for specialized short-term analysis
   - **Weekly** (~1% determination rate) - for high-frequency monitoring

2. **Flexible weight calibration**: Adjusts survey weights for sub-quarterly estimates
   - Calibrates to official monthly population totals (SIDRA API)
   - Adaptive hierarchical raking (4/2/1 cell levels depending on sample size)
   - Adaptive smoothing to remove quarterly artifacts (3/7/none periods)

3. **Dual data format support**: Works with both quarterly and annual PNADC releases
   - **Quarterly data** (V1028 weights) - labor market variables, all rotation groups
   - **Annual data** (V1032 weights) - comprehensive income (VD5008), single visit

### Why This Matters

Policy questions often require sub-quarterly precision that raw PNADC data cannot provide:

- When exactly did unemployment spike during COVID-19? (March 2020 vs. January 2020)
- How quickly did labor markets respond to minimum wage changes? (month-to-month transitions)
- What was poverty in December 2023 specifically? (not just Q4 2023 average)

The algorithm exploits PNADC's rotating panel design and birthday information to recover this temporal precision. By identifying when each household was actually interviewed within the quarter, we can construct monthly time series with 97% of the original sample retained.

### What You'll Learn

This vignette walks through the complete methodology:

1. **Why stacked multi-quarter data matters** - How cross-quarter aggregation improves determination from ~70% to ~97%
2. **The core algorithm** (8-step pipeline):
   - Valid interview date calculation (IBGE timing rules)
   - Birthday constraints to narrow date ranges
   - UPA-panel aggregation (households interviewed together)
   - Cross-quarter constraint propagation
   - Dynamic exception detection for edge cases
3. **Weight calibration approaches**:
   - Quarterly data: redistribute quarterly totals across 3 months
   - Annual data: redistribute yearly totals across 12 months
   - Fortnight/weekly data: simplified raking for smaller samples
4. **Performance characteristics** - Determination rates, processing speed, data quality
5. **Best practices** - Input data requirements, handling indeterminate cases, validation

### Prerequisites

This vignette assumes you're already familiar with basic PNADC concepts (UPA, rotation groups, survey weights) and have seen the package in action via the [Get Started](getting-started.html) vignette. We focus here on **understanding the methodology** rather than **how to use the functions**.

If you're looking for applied examples with real policy questions, see the [Applied Examples](applied-examples.html) vignette. For poverty analysis using annual PNADC data, see [Annual Poverty Analysis](annual-poverty-analysis.html).

---

## Quick Reference

### Required Input Variables

**Minimum (for reference month identification):**

| Column | Description |
|--------|-------------|
| `Ano` | Survey year |
| `Trimestre` | Quarter (1-4) |
| `UPA` | Primary Sampling Unit |
| `V1014` | Panel identifier |
| `V1008` | Household identifier |
| `V2003` | Person identifier |
| `V2008` | Birth day (1-31, or 99 for unknown) |
| `V20081` | Birth month (1-12, or 99 for unknown) |
| `V20082` | Birth year (or 9999 for unknown) |
| `V2009` | Age |

**Additional (for `calibrate = TRUE` in `pnadc_apply_periods()`):**

| Variable | Description |
|----------|-------------|
| `V1028` | Original quarterly survey weight |
| `V1032` | Original annual survey weight (for annual data only) |
| `UF` | State code |
| `posest` | Post-stratification cell |
| `posest_sxi` | Post-stratification group |

### Output Variables

| Variable | Type | Description |
|----------|------|-------------|
| `ref_month` | Date | First day of reference month (e.g., "2023-01-01") |
| `ref_month_in_quarter` | Integer | Position: 1, 2, 3, or NA if indeterminate |
| `ref_month_yyyymm` | Integer | YYYYMM format (e.g., 202301) |
| `weight_monthly` | Numeric | Monthly weight (if `calibrate = TRUE`); NA for indeterminate |

### Functions

| Function | Description |
|----------|-------------|
| `pnadc_identify_periods()` | Main function: build crosswalk with months/fortnights/weeks |
| `pnadc_apply_periods()` | Apply crosswalk + calibrate weights (quarterly or annual) |
| `identify_reference_month()` | Just reference month identification |
| `identify_reference_fortnight()` | Fortnight (quinzena) identification |
| `identify_reference_week()` | Week identification |
| `calibrate_monthly_weights()` | Rake weighting for monthly weights (quarterly data) |
| `fetch_monthly_population()` | Fetch population from SIDRA API |
| `validate_pnadc()` | Input data validation |

### Performance Summary

| Metric | Result |
|--------|--------|
| Monthly determination rate | **97.0%** (full history, 2012-2025) |
| Fortnight determination rate | **3.0%** |
| Weekly determination rate | **0.9%** |
| Processing time (basic mode) | **~1 minute** for 28.4M rows (~450,000 rows/sec) |
| Processing time (with weights) | **~5 minutes** |
| Best period (2013-2019) | 97.7% monthly determination |
| COVID period (2020-2021) | 94.7% monthly determination |

---

## Quarterly vs. Annual PNADC Data

PNADC has two main data releases that require different approaches:

| Aspect | Quarterly Data | Annual Data |
|--------|---------------|-------------|
| **Primary focus** | Labor market (employment, unemployment) | Income and poverty |
| **Key income variable** | Limited (labor income only) | VD5008 (comprehensive household income) |
| **Weight variable** | V1028 (quarterly weight) | V1032 (annual/visit-specific weight) |
| **Panel coverage** | All 5 rotation groups mixed | One specific visit (e.g., visit 1 or visit 5) |
| **Monthly observations** | ~20% from each visit | 100% from single visit |
| **Function** | `pnadc_apply_periods(..., anchor="quarter")` | `pnadc_apply_periods(..., anchor="year")` |

**Why does this matter?**

- **Reference month identification** works the same for both data types (birthday constraints don't depend on weight variables)
- **Weight calibration** is fundamentally different:
  - Quarterly weights (V1028) assume all 5 rotation groups are present
  - Annual weights (V1032) are calibrated for visit-specific estimates
  - Using quarterly `weight_monthly` with annual data would misrepresent the population

For income and poverty analysis using annual PNADC data, see the [Annual Poverty Analysis](annual-poverty-analysis.html) vignette.

---

## Why Stacked Data Matters

The algorithm achieves **97% determination rate** when processing **stacked multi-quarter data**. If you process quarters individually, you'll only get ~73% determination.

The figure below shows how determination rate improves as more quarters are stacked, computed from real PNADC data (2012-2025):

![Determination rate improves with more stacked quarters](figures/how-it-works/fig-determination-rate.png){width=100%}

**Empirical determination rates by number of quarters stacked:**

| Quarters Stacked | Determination Rate |
|------------------|-------------------|
| 1 (single quarter) | 73.0% |
| 2 | 88.2% |
| 4 (1 year) | 93.9% |
| 8 (2 years) | 96.3% |
| 12 (3 years) | 97.1% |
| 20 (5 years) | 97.3% |
| 32 (8 years) | 97.4% |
| 55 (full history) | **97.0%** |

Note that the rate peaks around 32 quarters and then stabilizes. This is because the boundary quarters (first 4 and last 4 of any series) have inherently lower rates due to incomplete panel coverage.

**Why does stacking help?** PNADC uses a **rotating panel** where each household (UPA + V1014) is interviewed for 5 consecutive quarters. Crucially, the same household is always interviewed in the **same relative month position** (always month 1, always month 2, or always month 3).

This means birthday constraints from **any quarter** can determine the month for **all quarters**:

```
UPA=123456, V1014=1 appears in 5 quarters:

  2023-Q1: month_min=1, month_max=2 (ambiguous: Jan or Feb)
  2023-Q2: month_min=1, month_max=3 (ambiguous: Apr, May, or Jun)
  2023-Q3: month_min=2, month_max=2 (DETERMINED: August)  ← Birthday constraint!
  2023-Q4: month_min=1, month_max=2 (ambiguous: Oct or Nov)
  2024-Q1: month_min=2, month_max=3 (ambiguous: Feb or Mar)

Cross-quarter aggregation:
  upa_month_min = max(1, 1, 2, 1, 2) = 2
  upa_month_max = min(2, 3, 2, 2, 3) = 2

Result: ALL 5 quarters → ref_month_in_quarter = 2
```

**Recommended**: Stack at least 2 years (8 quarters) of data before calling `pnadc_identify_periods()` to achieve >96% determination.

---

## The Algorithm Explained

### Overview Diagram

The reference month identification follows an 8-step pipeline:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    REFERENCE MONTH IDENTIFICATION PIPELINE                  │
└─────────────────────────────────────────────────────────────────────────────┘

INPUTS (per observation):
┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│     Year     │  │    Quarter   │  │    Birthday  │  │      Age     │
│    (Ano)     │  │  (Trimestre) │  │  (V2008/1/2) │  │    (V2009)   │
└──────────────┘  └──────────────┘  └──────────────┘  └──────────────┘
        │                │                  │                 │
        ▼                ▼                  ▼                 ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 1a: Calculate Valid Interview Weeks                                    │
│          (IBGE "Parada Tecnica" Rules)                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│  IBGE Rule: A reference week belongs to a month only if it has at least     │
│  4 days within the month.                                                   │
│                                                                             │
|  So, for finding the first reference week of IBGE interviews find the       |
|  Saturday of that month and count how many days it has been since 1st day.  |
│                                                                             │
|  If ≥4 days, that is considered a valid reference week.                     |
|  If less than 3 days, that short week will be considered a "technical stop" | 
|  Then skip to the next (second) Saturday                                    |
│                                                                             │
│  Saturdays are the reference day of each reference week. They are the dates |
|  the IBGE interviewers refer to in questions involving precise timing. So   │
│  the first Saturday of each month is also the first valid interview date    │
│  for that month                                                             │
│                                                                             │
│  OUTPUT: First reference day of Months 1, 2, and 3 of the quarter           │
└─────────────────────────────────────────────────────────────────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 1b: Calculate the range of possible interview dates for each quarter   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Interviews (i.e. valid reference dates for interviews) within a quarter    │
│  can only have happened from the first valid Saturday of Month 1 up to the  │
│  last Saturday of Month 3 -- i.e. First Saturday of Month 3 + 21 days       │
│  (since reference weeks end on Saturdays). This gives us date bounds.       │   
│                                                                             │
│  OUTPUT: date_min = First Saturday of Month 1                               │
│          date_max = First Saturday of Month 3 + 21 days                     │
└─────────────────────────────────────────────────────────────────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 2: Apply Birthday Constraints                                          │
├─────────────────────────────────────────────────────────────────────────────┤
│  Compare (Survey_Year - Birth_Year) with Reported_Age to determine if       │
│  interview was BEFORE or AFTER the person's birthday this year.             │
│                                                                             │
|  The goal is to Narrow the possible interview date window using each        |
|  person’s birthday and calculated age (by IBGE on the reference date of     |
|  the interview, which is the Saturday that ends the reference week).        |│                                                                             │
│  If AFTER birthday  -> date_min = max(date_min, Sat_after_birthday)         │
│  If BEFORE birthday -> date_max = min(date_max, Sat_before_birthday)        │
└─────────────────────────────────────────────────────────────────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 3: Convert Date Ranges to Month Positions (1, 2, or 3)                 │
├─────────────────────────────────────────────────────────────────────────────┤
│  month_min_pos = which month does date_min fall in?                         │
│  month_max_pos = which month does date_max fall in?                         │
│                                                                             │
│  Special handling: Weeks in which Saturdays fall on the 1st to 3rd day of   │
│  a month are assigned to the previous month.                                │
└─────────────────────────────────────────────────────────────────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 4: Aggregate to UPA-Panel Level                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│  All people in same (UPA, V1014) are interviewed in the SAME MONTH.         │
│  Take intersection of all individual constraints:                           │
│                                                                             │
│    upa_month_min = max(all individual month_min_pos)                        │
│    upa_month_max = min(all individual month_max_pos)                        │
│                                                                             │
│  If upa_month_min = upa_month_max -> DETERMINED!                            │
│  If upa_month_min < upa_month_max -> Still ambiguous, try Step 5            │
└─────────────────────────────────────────────────────────────────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 5: Cross-Quarter Aggregation                                           │
├─────────────────────────────────────────────────────────────────────────────┤
│  PNADC uses ROTATING PANEL: same (UPA, V1014) interviewed in SAME MONTH     │
│  POSITION across all quarterly visits (up to 5 consecutive quarters).       │
│                                                                             │
│  Combine constraints from ALL quarters -> dramatically improves accuracy:   │
│                                                                             │
│    Per-quarter only:  ~65-75% determination                                 │
│    Cross-quarter:     ~97.0% determination                                  │
└─────────────────────────────────────────────────────────────────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 6: Dynamic Exception Detection                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│  Algorithm DYNAMICALLY detects which quarters need relaxed timing rules:    │
│                                                                             │
│  1. Calculate positions using STANDARD rules (>=4 days, day <=3 threshold)  │
│  2. Calculate positions using ALTERNATIVE rules (>=3 days, day <=2)         │
│  3. Detect: if standard produces min>max but alternative would work         │
│  4. Propagate: if ANY UPA needs exception -> apply to ALL in that quarter   │
│                                                                             │
│  Exception quarters are detected AUTOMATICALLY by the algorithm.            │
│  Examples found in 2012-2025: 2016q3, 2016q4, 2017q2, 2022q3, 2023q2, etc.  │
└─────────────────────────────────────────────────────────────────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 7: Final Month Assignment                                              │
├─────────────────────────────────────────────────────────────────────────────┤
│  After applying exceptions and recalculating:                               │
│  If upa_month_min == upa_month_max -> Reference month DETERMINED            │
│  Otherwise -> remains NA (indeterminate, ~3% of observations)               │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ OUTPUT                                                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│  ref_month_in_quarter: 1, 2, 3, or NA (indeterminate)                       │
│  ref_month:            Full date (e.g., 2023-02-01)                         │
│  ref_month_yyyymm:     Integer format (e.g., 202302)                        │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### Step 1: Valid Interview Saturdays (IBGE Rules)

As previously stated, IBGE defines that a reference week belongs to a month only if it has at least **4 days within that month**. Since reference weeks end on Saturdays, we look at the Saturdays of each month to determine valid interview dates.

**How we calculate the first valid Saturday:**

```
first_saturday_day = day of the first Saturday of the month
if first_saturday_day >= 4:
    use this Saturday (it has enough days in the month)
else:
    use the second Saturday (first_saturday_day + 7)
```

**Example for Q1 2023:**

```
JANUARY 2023:
  Sun  Mon  Tue  Wed  Thu  Fri  SAT
   1    2    3    4    5    6   [7]  <- First Saturday is day 7
                                     7 >= 4? YES -> Valid! (7 days in January)

FEBRUARY 2023:
  Wed  Thu  Fri  SAT  ...
   1    2    3   [4]              <- First Saturday is day 4
                                  4 >= 4? YES -> Valid! (4 days in February)

APRIL 2023:
  SAT  Sun  Mon  Tue  ...
  [1]   2    3    4              <- First Saturday is day 1
                                  1 >= 4? NO -> Skip to second Saturday!
  Fri  SAT  Sun  ...
   7   [8]   9                   <- Use day 8 instead
```

The valid Saturday calculation defines the **possible interview date range** for the quarter:
- `date_min` = First valid Saturday of Month 1
- `date_max` = First valid Saturday of Month 3 + 21 days

---

### Step 2: Birthday Constraints

IBGE calculates age precisely using the precise birthdate and the interview reference date (and most persons inform birthdates). In other words, IBGE **calculates age on the Saturday** that ends the reference week of the interview. 

So, comparing the calculated age to the birth year, we can determine if the interview (i.e. the reference date) happened before or after the person's birthday that year. This allows us to further narrow down the possible interview date window.

```
visit_before_birthday = (Survey_Year - Birth_Year) - Calculated_Age

If = 0: Interview was AFTER birthday (person already celebrated this year)
If = 1: Interview was BEFORE birthday (birthday hasn't happened yet)
```

**Example: Interview AFTER birthday**

```
Person: Born March 15, 1990
Survey Year and quarter: 2023 Q1
Calculated Age: 33

Check age: 2023 - 1990 = 33
           33 - 33 = 0  -> Interview was AFTER March 15, 2023

Check first Saturday on or after birthday:
           first_Saturday_on_or_after(March 15) = March 18, 2023

New date constraints: 
            date_min = max(date_min, first_Saturday_on_or_after(March 15))                        
            date_min = max(Jan 7, March 18) = March 18

This person can only have been interviewed in MARCH (Month 3 of Q1).
```

**Example: Interview BEFORE birthday**

```
Person: Born February 17, 1990
Survey Year and quarter: 2023 Q1
Calculated Age: 32

Check age: 2023 - 1990 = 33
           33 - 32 = 1  -> Interview was BEFORE February 17, 2023

Check first Saturday on or after birthday:
           first_Saturday_on_or_after(February 17) = February 18, 2023 (Sat)
           Saturday_before = February 18 - 7 = February 11, 2023

Constraint: date_max = min(date_max, Saturday_before(birthday))
            date_max = min(March 25, February 11) = February 11

This person can only have been interviewed in JANUARY or early FEBRUARY.
```

**Unknown birthdays**: When V2008=99, V20081=99, or V20082=9999, that person's birthday cannot constrain the date. However, they may still be determined through aggregation by UPA-Panel level (Step 4) or cross-quarter aggregation (Step 5).

---

### Step 3: Date to Period Position

Transform the date window [date_min, date_max] into period positions. This step works similarly for all three granularities:

**Month positions (1, 2, or 3):**

```
For date_min: if day <= 3 AND not in first month of quarter:
              subtract 1 from month position

For date_max: if day <= 3:
              use the month of (date - 3 days) instead
```

**Boundary handling**: Interviews on days 1-3 of a month typically belong to a reference week that started in the previous month.

**Fortnight positions (1-6):**

After determining the month position, fortnights are calculated as:
```
fortnight_pos = (month_pos - 1) * 2 + half_of_month
  where half_of_month = 1 if day <= 15, else 2
```

| Month | Days 1-15 | Days 16-31 |
|-------|-----------|------------|
| 1 | Fortnight 1 | Fortnight 2 |
| 2 | Fortnight 3 | Fortnight 4 |
| 3 | Fortnight 5 | Fortnight 6 |

**Week positions (ISO week number):**

Weeks use the ISO 8601 standard for consistent numbering:
```
week_yyyyww = YYYY * 100 + ISO_week_number
```

ISO weeks:
- Start on Monday
- Week 1 is the first week containing at least 4 days of the new year
- Can span year boundaries (e.g., December 30 might be in week 1 of next year)

The resulting period positions feed into Step 4 (aggregation), which differs by granularity (see "Fortnight and Week Identification" below for details).

---

### Step 4: UPA-Panel Aggregation (Months)

All people in the same **UPA + V1014** are interviewed together in the **same reference month**. Take the intersection:

```
upa_month_min = MAX of all individual month_min_pos
upa_month_max = MIN of all individual month_max_pos
```

**Example:**

```
A household located at UPA=123456 and member of the Panel group V1014=1 has 3 members:

Person A: month_min=1, month_max=2 (could be Jan or Feb)
Person B: month_min=1, month_max=3 (could be Jan, Feb, or Mar)
Person C: month_min=2, month_max=3 (could be Feb or Mar)

Aggregation:
  upa_month_min = max(1, 1, 2) = 2
  upa_month_max = min(2, 3, 3) = 2

Result: min=2, max=2 -> Reference month is FEBRUARY!

Visual:
              Jan    Feb    Mar
Person A:     [=========]
Person B:     [============]
Person C:            [========]
Intersection:        [==]      <- Only February satisfies all
```

**Note on fortnights and weeks**: Fortnights and weeks also aggregate constraints at Step 4, but using different keys: `(Ano, Trimestre, UPA, V1008)` (household within a specific quarter) instead of `(UPA, V1014)` (panel across quarters). This is because the specific fortnight or week can vary between quarters even for the same household. See "Fortnight and Week Identification" below for details.

---

### Step 5: Cross-Quarter Aggregation (Months Only)

Since the relative month position is constant across quarters, constraints from **any** quarter apply to **all** quarters:

```
For each (UPA, V1014) group across ALL quarters:
  upa_month_min = MAX of all month_min_pos from all quarters
  upa_month_max = MIN of all month_max_pos from all quarters
```

This is why processing stacked data improves determination from ~70% to ~97%.

**Critical distinction**: This cross-quarter aggregation is what makes monthly determination so effective (~97%), but it **cannot be applied to fortnights or weeks**. Even though a household is always interviewed in the same relative month (e.g., always month 2), the specific fortnight or week within that month varies by quarter. Therefore, fortnight and week constraints must be evaluated within each quarter independently, resulting in much lower determination rates (~2-5% for fortnights, ~1-2% for weeks).

---

### Step 6: Dynamic Exception Detection

In some quarters, the standard rule (first reference week with ≥4 days) may produce impossible results. For instance, `upa_month_min > upa_month_max`. Quarters in which this happens are flagged as needing an exception. We then implement an **alternative rule**: first reference week with ≥3 days

**How dynamic detection works**: The algorithm calculates month positions using both rules simultaneously:

| Rule | First reference week | Day Threshold |
|------|----------------------|---------------|
| Standard | >=4 days in month | day <= 3 |
| Alternative | >=3 days in month | day <= 2 |

Exceptions are detected and applied at the month level within each quarter. For example, in 2016q3, only month 3 (September) needs the exception -- months 1 and 2 use the standard rule. Here's why: under the standard rule, September's first valid Saturday would be September 10th (since September 3rd has only 3 days in the month). But some UPA-V1014 groups have constraints that require September 3rd to be valid. The exception rule (≥3 days instead of ≥4) makes September 3rd valid for these groups, assigning the week of August 28th-September 3rd to September's reference month rather than August's.

And the algorithm proceeds as follows:

```
For each UPA-V1014:
  1. Calculate upa_month_min/max using STANDARD rules
  2. Also calculate alt_upa_month_min/max using ALTERNATIVE rules
  3. Check if exception is needed. ALL conditions must be true:
     a) Standard produces impossible result: upa_month_min > upa_month_max
     b) Alternative would work: alt_upa_month_min <= alt_upa_month_max
     c) Individual's constraint is the binding one that could be relaxed
        (the individual's bound equals the UPA bound that would change)
  4. Determine WHICH month within the quarter needs the exception:
     - requires_exc_m1: Exception needed for month 1
     - requires_exc_m2: Exception needed for month 2
     - requires_exc_m3: Exception needed for month 3
  5. PROPAGATE: If ANY UPA in the quarter needs exception for a month,
     apply that exception to ALL observations in that quarter
```

Exception quarters are detected automatically. Examples found in 2012-2025 data: 2016q3, 2016q4, 2017q2, 2022q3, 2023q2, 2024q1. The algorithm will detect any new exceptions in future data.

---

### Step 7: Final Assignment

After applying exceptions: if `upa_month_min == upa_month_max`, the reference month is determined. Otherwise, it remains NA.

**What makes observations indeterminate (~3%)?**

This occurs when the available birth date information in a (UPA, V1014) group is insufficient to narrow down the possibilities to a single month. Several factors contribute:

- **Incomplete panel coverage**: Some UPAs may have fewer than the expected 5 visits in the dataset. The first and last 4 quarters of any consecutive series will naturally have more UPAs with incomplete visit histories due to the rotating panel design.
- **Unit non-response**: The survey targets 14 households per UPA each quarter, but non-response reduces this. Fewer responding households means fewer birthday constraints to narrow down the month.
- **Small household sizes**: Households with fewer members provide fewer birthday constraints.
- **Missing birth dates**: Not all respondents report their date of birth. Higher item non-response reduces the available constraints.
- **Uninformative birthdays**: Some reported birth dates don't help distinguish between months (e.g., birthdays outside the quarter's interview window).

Indeterminate observations are then removed from monthly analysis (if `keep_all = FALSE`) or assigned `month = NA` and `weight_monthly = NA` (if `keep_all = TRUE`).

---

## Fortnight and Week Identification

The package also identifies **fortnights** (quinzenas) and **weeks** within each quarter. These use the same core methodology as months but with important differences in how constraints are aggregated.

### Why Lower Determination Rates?

| Period | Determination Rate | Aggregation Scope |
|--------|-------------------|-------------------|
| **Month** | ~97% | Cross-quarter (UPA, V1014) |
| **Fortnight** | ~3% | Within-quarter (Ano, Trimestre, UPA, V1008) |
| **Week** | ~1% | Within-quarter (Ano, Trimestre, UPA, V1008) |

The key difference: **months benefit from cross-quarter aggregation** (Step 5), while fortnights and weeks cannot.

**Why can't fortnights/weeks aggregate across quarters?**

- A household's **relative month position** is constant: if interviewed in month 2 of Q1, they're always interviewed in month 2 of every quarter.
- But the **specific fortnight or week** varies: even if always in month 2, they might be in fortnight 3 in Q1 but fortnight 4 in Q2.
- Therefore, fortnight/week constraints from different quarters cannot be combined.

### Fortnight Identification Algorithm

Fortnights divide the quarter into 6 periods (2 per month):

```
Quarter structure:
Month 1: Fortnight 1, Fortnight 2
Month 2: Fortnight 3, Fortnight 4
Month 3: Fortnight 5, Fortnight 6
```

The algorithm follows the same Steps 1-4 as months, but:

**Step 3 (Fortnight version)**: Convert dates to fortnight positions (1-6)

```
For each date in the quarter:
  1. Determine which month (1, 2, or 3)
  2. Determine if first or second half of month:
     - Days 1-15: First fortnight
     - Days 16+: Second fortnight
  3. Calculate position: (month - 1) * 2 + half
```

**Step 4 (Fortnight version)**: Aggregate at household level **within quarter**

```
For each (Ano, Trimestre, UPA, V1008) group:
  hh_fortnight_min = max(all individual fortnight_min_pos)
  hh_fortnight_max = min(all individual fortnight_max_pos)

If hh_fortnight_min == hh_fortnight_max -> DETERMINED
```

**No Step 5**: Fortnights cannot use cross-quarter aggregation.

**Output variables:**

| Variable | Description |
|----------|-------------|
| `ref_fortnight` | First day of reference fortnight |
| `ref_fortnight_in_quarter` | Position 1-6, or NA if indeterminate |
| `ref_fortnight_yyyyff` | YYYYFF format (e.g., 202303 = 3rd fortnight of 2023, i.e., Feb 1-15) |
| `determined_fortnight` | Logical: TRUE if determined |

### Week Identification Algorithm

Weeks divide the quarter into approximately 13 periods:

```
Quarter structure (example):
Month 1: Weeks 1-4 (or 5, depending on calendar)
Month 2: Weeks 5-8 (or 4-8)
Month 3: Weeks 9-13 (or 8-13)
```

The algorithm uses ISO week numbers for consistency:

**Step 3 (Week version)**: Convert dates to ISO week numbers

```
For each date:
  ref_week_yyyyww = ISO week number (YYYYWW format)

ISO weeks start on Monday and can span year boundaries.
For example, week 1 of 2024 might include days from late December 2023.
```

**Step 4 (Week version)**: Aggregate at household level **within quarter**

```
For each (Ano, Trimestre, UPA, V1008) group:
  hh_week_min = max(all individual week_min_yyyyww)
  hh_week_max = min(all individual week_max_yyyyww)

If hh_week_min == hh_week_max -> DETERMINED
```

**Output variables:**

| Variable | Description |
|----------|-------------|
| `ref_week` | First day (Monday) of reference week |
| `ref_week_in_quarter` | Position 1-13, or NA if indeterminate |
| `ref_week_yyyyww` | ISO week format (e.g., 202315 = week 15 of 2023) |
| `determined_week` | Logical: TRUE if determined |

### Practical Implications

**When to use each granularity:**

| Granularity | Sample Size | Use Cases |
|-------------|-------------|-----------|
| **Monthly** | ~97% of data | Most analyses: labor market trends, poverty dynamics |
| **Fortnight** | ~3% of data | Short-term shock analysis (requires large initial sample) |
| **Weekly** | ~1% of data | High-frequency monitoring (limited reliability) |

**Important considerations:**

1. **Sample size decreases rapidly**: With only ~3% fortnight or ~1% week determination, you need very large initial datasets to have meaningful sub-monthly samples.

2. **Not a filter problem**: The low determination rates are inherent to the methodology, not a bug. Birthday constraints within a single quarter rarely narrow the interview date to a specific fortnight or week.

3. **Weight calibration adapts**: The package automatically uses simplified raking (fewer cell levels) for fortnights/weeks to handle smaller samples.

### Usage Example

```{r fortnight-week-identification}
# Build crosswalk with all period types
crosswalk <- pnadc_identify_periods(pnadc_stacked, verbose = TRUE)

# Check determination rates
crosswalk[, .(
  n_obs = .N,
  det_month = mean(determined_month, na.rm = TRUE),
  det_fortnight = mean(determined_fortnight, na.rm = TRUE),
  det_week = mean(determined_week, na.rm = TRUE)
), by = .(Ano, Trimestre)]

# For fortnight analysis, filter to determined observations
fortnight_data <- crosswalk[determined_fortnight == TRUE]
nrow(fortnight_data) / nrow(crosswalk)  # ~3%

# Apply crosswalk with fortnight calibration
result <- pnadc_apply_periods(
  pnadc_data, crosswalk,
  weight_var = "V1028",
  anchor = "quarter",
  calibration_unit = "fortnight"
)
```

---

## Monthly Weights

For monthly aggregate estimates, you need monthly-appropriate survey weights.

### How Weight Calibration Works

When `calibrate = TRUE` in `pnadc_apply_periods()`, the package:

**1. Fetches monthly population from SIDRA API** (table 6022)

SIDRA provides **moving-quarter** population estimates, not exact monthly values:

| SIDRA Code | 3-Month Window | Represents Population For |
|------------|----------------|---------------------------|
| 201203 | Jan+Feb+Mar 2012 | **February 2012** |
| 201204 | Feb+Mar+Apr 2012 | **March 2012** |
| 201205 | Mar+Apr+May 2012 | **April 2012** |

The package shifts values to align with their center month. Boundary months (Jan 2012, latest month) are extrapolated via quadratic regression.

**2. Applies hierarchical rake weighting** across nested calibration cells:

| Cell Level | Definition | Purpose |
|------------|------------|---------|
| `celula1` | Age groups: 0-13, 14-29, 30-59, 60+ | Demographic balance |
| `celula2` | Post-stratum group + age | Regional-demographic balance |
| `celula3` | State (UF) + celula2 | State-level balance |
| `celula4` | Post-stratum (posest) + celula2 | Fine geographic balance |

The number of cell levels used depends on the calibration unit (see "Fortnight and Weekly Calibration" below).
At each level, weights are adjusted so period totals match anchor totals proportionally.

**3. Calibrates to monthly population totals**

Final scaling ensures monthly weighted totals match SIDRA population (~206 million average).

### Usage Example

```{r monthly-weights}
# Load full data with all required variables
pnadc_full <- fread("pnadc_stacked_full.csv")

# Step 1: Build crosswalk
crosswalk <- pnadc_identify_periods(pnadc_full, verbose = TRUE)

# Step 2: Apply crosswalk and calibrate weights
result <- pnadc_apply_periods(pnadc_full, crosswalk,
                               weight_var = "V1028",
                               anchor = "quarter",
                               calibrate = TRUE,
                               verbose = TRUE)

# Use weight_monthly for monthly estimates
monthly_pop <- result[, .(
  population = sum(weight_monthly, na.rm = TRUE)
), by = ref_month_yyyymm]
```

### Fortnight and Weekly Calibration

The package also supports calibrating weights for fortnight and weekly analysis using `calibration_unit = "fortnight"` or `calibration_unit = "week"`.

**Population Targets**

All time periods are calibrated to the **full Brazilian population** from SIDRA:

| Calibration Unit | Population Target |
|------------------|-------------------|
| Month | SIDRA monthly population (~206 million) |
| Fortnight | Full monthly population of the containing month |
| Week | Full monthly population of the containing month |

This means that if you sum `weight_fortnight` for a given fortnight, or `weight_weekly` for a given week, you get the total Brazilian population for that period (not a fraction of it).

**Simplified Hierarchical Raking**

Because fortnights and weeks have much lower determination rates (~2-5% and ~1-2% respectively), the hierarchical raking is automatically simplified to avoid sparse cell issues:

| Calibration Unit | Det. Rate | Cell Levels Used | Rationale |
|------------------|-----------|------------------|-----------|
| Month | ~97% | 4 levels (celula1-4) | ~65,000+ obs/period allows fine cells |
| Fortnight | 2-5% | 2 levels (celula1-2) | ~3,000 obs/period; finer cells too sparse |
| Week | 1-2% | 1 level (celula1 only) | ~500 obs/period; only age groups stable |

The algorithm also checks minimum cell sizes (default: 10 observations) and skips raking levels if cells become too sparse.

**Smoothing Behavior**

Smoothing to remove quarterly artifacts is also adapted:

| Calibration Unit | Smoothing Window | Cell Level |
|------------------|------------------|------------|
| Month | 3-period rolling mean | celula4 |
| Fortnight | 7-period rolling mean | celula2 |
| Week | No smoothing | N/A |

Weekly data skips smoothing entirely because the small sample sizes make cell-level smoothing unstable.

**Usage Example**

```{r fortnight-weekly}
# Fortnight calibration
result <- pnadc_apply_periods(pnadc_data, crosswalk,
                               weight_var = "V1028",
                               anchor = "quarter",
                               calibration_unit = "fortnight")

# Weekly calibration
result <- pnadc_apply_periods(pnadc_data, crosswalk,
                               weight_var = "V1028",
                               anchor = "quarter",
                               calibration_unit = "week")
```

### Handling Indeterminate Observations

By default (`keep_all = TRUE`), all input rows are returned, with `weight_monthly = NA` for indeterminate observations:

```{r keep-all-example}
# Build crosswalk and apply (returns all rows by default)
crosswalk <- pnadc_identify_periods(pnadc_full)
result <- pnadc_apply_periods(pnadc_full, crosswalk,
                               weight_var = "V1028",
                               anchor = "quarter")

nrow(result) == nrow(pnadc_full)  # TRUE - all rows returned
sum(is.na(result$weight_monthly))  # ~3% of rows have NA weights

# Use na.rm = TRUE when aggregating
monthly_pop <- result[, .(pop = sum(weight_monthly, na.rm = TRUE)), by = ref_month_yyyymm]

# Filter to determined rows for analysis
result_determined <- result[!is.na(ref_month_in_quarter)]
nrow(result_determined) < nrow(pnadc_full)  # TRUE (~97% of rows)
```

---

## Weight Smoothing

During weight calibration, the package can optionally apply smoothing to reduce artificial quarterly patterns in the calibrated weights. This is controlled by the `smooth` parameter in `pnadc_apply_periods()` (default: `TRUE`).

### Why Weight Smoothing Helps

PNADC's quarterly structure creates subtle artifacts in monthly estimates:

1. **Rotation group composition varies by month**: Each quarter has 5 rotation groups, but their distribution across months isn't perfectly even. Some months may have slightly more observations from groups with different demographic compositions.

2. **Boundary effects**: The first and last months of a quarter can have different characteristics than middle months due to how interviews are scheduled around "Parada Tecnica" (technical breaks).

3. **Cell-level imbalances**: Within demographic cells (age × region × stratum), the month-to-month sample sizes can vary, creating artificial jumps even when the true population is stable.

Weight smoothing addresses these issues by applying a rolling mean to the calibration adjustments at the cell level, reducing artificial volatility while preserving genuine trends.

### How Weight Smoothing Works

During calibration, the package applies rolling mean smoothing to cell-level population ratios:

```
For each (cell, period):
  1. Calculate cell population: pop_cell_period = sum(weights)
  2. Apply rolling mean across periods within cell:
     pop_smoothed = frollmean(pop_cell_period, window, align="center")
  3. Calculate smoothing factor: factor = pop_smoothed / pop_cell_period
  4. Adjust individual weights: weight_new = weight_old * factor
```

The smoothing window adapts by time period granularity:

| Period | Window | Rationale |
|--------|--------|-----------|
| Month | 3 periods | Captures quarter-to-quarter transitions |
| Fortnight | 7 periods | Wider window for more volatile estimates |
| Week | None | Sample too small for stable smoothing |

### When to Disable Smoothing

By default, `smooth = TRUE` is recommended for most analyses. However, you may want to disable smoothing (`smooth = FALSE`) in certain situations:

**Disable smoothing when:**

- **Analyzing rapid shocks**: When studying events with genuine month-to-month discontinuities (e.g., COVID-19 lockdown effects in March 2020, sudden policy changes), smoothing may blur the true timing of the effect.

- **Studying short-term dynamics**: When the research question focuses on immediate month-to-month changes rather than trends.

- **Comparing specific months**: When comparing the same month across different years (e.g., December 2022 vs December 2023), smoothing may introduce bias from adjacent months.

**Keep smoothing enabled when:**

- Computing time series for publication or visualization
- Analyzing gradual trends (employment growth, demographic shifts)
- The underlying phenomenon is expected to change smoothly over time

```{r smoothing-options}
# With smoothing (default) - recommended for most analyses
result <- pnadc_apply_periods(
  pnadc_data, crosswalk,
  weight_var = "V1028",
  anchor = "quarter",
  smooth = TRUE  # default
)

# Without smoothing - for shock analysis
result_unsmoothed <- pnadc_apply_periods(
  pnadc_data, crosswalk,
  weight_var = "V1028",
  anchor = "quarter",
  smooth = FALSE
)
```

**Note**: Smoothing is applied to the microdata weights during calibration. If you still observe quarterly artifacts in your final aggregated estimates, this may indicate genuine sampling variation rather than calibration artifacts.

---

## Mensalizing Annual PNADC Data

Annual PNADC data (with comprehensive income variables like VD5008) requires a different approach than quarterly data.

### Why Annual Data Needs Different Weight Calibration

The quarterly weight calibration (`calibrate_monthly_weights()`) makes assumptions that don't hold for annual data:

| Assumption | Quarterly Data | Annual Data |
|------------|---------------|-------------|
| **Panel composition** | All 5 rotation groups present | Only 1 specific visit |
| **Starting weight** | V1028 (quarterly weight) | V1032 (annual weight) |
| **Calibration anchor** | Quarterly totals | Yearly totals |

**The problem**: If you use quarterly `weight_monthly` directly with annual data, each observation would be weighted as if it represents only 20% of its demographic cell (since quarterly data has 5 rotation groups). Annual data has 100% from a single visit, so this approach severely underweights the population.

### How Annual Weight Calibration Works

For annual data, `pnadc_apply_periods()` with `anchor = "year"` follows this algorithm:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                 ANNUAL WEIGHT CALIBRATION PIPELINE                          │
└─────────────────────────────────────────────────────────────────────────────┘

STEP 1: Merge crosswalk with annual data
        ├── Annual data: contains V1032 weight, income variables (VD5008)
        └── Crosswalk: contains ref_month_in_quarter from quarterly data

STEP 2: Create demographic cells (same as quarterly)
        ├── celula1: Age groups (0-13, 14-29, 30-59, 60+)
        ├── celula2: Post-stratum group + age
        ├── celula3: State (UF) + celula2
        └── celula4: Post-stratum (posest) + celula2

STEP 3: Hierarchical rake reweighting (DIFFERENT from quarterly)
        ├── KEY DIFFERENCE: Use YEARLY totals as anchor (not quarterly)
        │
        │   For each (cell, year):
        │     pop_year = sum(V1032)
        │     n_months = number of months in cell-year
        │     expected_monthly = pop_year / n_months
        │
        │   For each (cell, month):
        │     pop_month = sum(weight_current)
        │     adjustment = expected_monthly / pop_month
        │     weight_new = weight_current * adjustment
        │
        └── Repeat for all 4 cell levels (hierarchical raking)

STEP 4: Final calibration to SIDRA monthly population totals (same as quarterly)
        └── Ensures weights sum to official monthly population (~206 million)

OUTPUT: weight_monthly (calibrated for annual data)
```

### Comparison: Quarterly vs. Annual Calibration

| Step | Quarterly Approach | Annual Approach |
|------|-------------------|-----------------|
| **Starting weight** | V1028 | V1032 |
| **Anchor for raking** | Quarterly totals | Yearly totals |
| **Redistribution** | Quarter → 3 months | Year → 12 months |
| **Final calibration** | SIDRA monthly population | SIDRA monthly population |

The key insight: Within each cell-year, the V1032 weights sum to that cell's annual population. We redistribute that total evenly across the months where that cell appears, then calibrate to SIDRA totals.

### Usage Example

```{r annual-workflow}
# Step 1: Create crosswalk from quarterly data
# (birthday variables are in quarterly data, not annual)
quarterly_data <- fread("pnadc_quarterly_stacked.csv")
crosswalk <- pnadc_identify_periods(quarterly_data)

# Step 2: Load annual data with income variables
annual_data <- fread("pnadc_annual_visit1_stacked.csv")

# Step 3: Apply crosswalk and calibrate annual weights
result <- pnadc_apply_periods(
  annual_data, crosswalk,
  weight_var = "V1032",
  anchor = "year",
  calibrate = TRUE,
  verbose = TRUE
)

# Step 4: Use weight_monthly for monthly income/poverty analysis
monthly_income <- result[, .(
  mean_income = weighted.mean(vd5008, weight_monthly, na.rm = TRUE)
), by = ref_month_yyyymm]
```

### COVID-Era Visit Selection

For annual data, you must choose which panel visit to use. During COVID-19 (2020-2021), visit 1 was often unavailable:

| Period | Recommended Visit | Reason |
|--------|------------------|--------|
| 2015-2019 | Visit 1 | Standard practice |
| 2020-2021 | Visit 5 | COVID-era data collection issues |
| 2022-2024 | Visit 1 | Post-pandemic normalization |

For a complete example of poverty analysis with annual data, see the [Annual Poverty Analysis](annual-poverty-analysis.html) vignette.

---

## Advanced Usage

### Using Modular Functions

For more control, use the individual functions:

```{r modular}
# Step 1: Just identify reference months
months <- identify_reference_month(pnadc)

# Step 2: Check determination rate by quarter
months[, .(
  total = .N,
  determined = sum(!is.na(ref_month_in_quarter)),
  rate = round(mean(!is.na(ref_month_in_quarter)) * 100, 1)
), by = .(Ano, Trimestre)]

# Step 3: Validate input data
validation <- validate_pnadc(pnadc, stop_on_error = FALSE)

# Step 4: Fetch population data from SIDRA-IBGE manually
pop_data <- fetch_monthly_population(verbose = TRUE)

# Step 5: For weight calibration, use pnadc_apply_periods() (recommended)
# The standalone calibrate_monthly_weights() requires manual data preparation
# and is intended for advanced use cases only
result <- pnadc_apply_periods(pnadc, months,
                               weight_var = "V1028",
                               anchor = "quarter",
                               calibrate = TRUE)
```

---

## Performance

### Benchmarks

| Dataset Size | Rows | Time (basic) | Throughput |
|--------------|------|--------------|------------|
| 1 quarter | ~570K | ~1.5 sec | ~380K rows/sec |
| 1 year | ~2.3M | ~5 sec | ~460K rows/sec |
| 14 years (2012-2025) | 28.4M | **~1 minute** | ~450K rows/sec |

**Key optimization**: The `make_date()` function uses pre-computed lookup tables instead of `ISOdate()`, achieving **20x speedup** on date operations.

### Determination Rates by Period

Based on real PNADC data (2012-2025, 55 quarters, 28.4M observations):

| Period | Quarters | Monthly Rate | Fortnight Rate | Week Rate |
|--------|----------|--------------|----------------|-----------|
| 2012 | Q1-Q4 | 91.4% - 98.8% | 3.2% - 4.8% | 1.2% |
| 2013-2019 | 28 quarters | 96.8% - 98.8% | 2.3% - 4.5% | 0.6% - 1.2% |
| 2020-2021 | 8 quarters | 92.7% - 97.3% | 1.9% - 2.9% | 0.5% - 1.4% |
| 2022-2024 | 15 quarters | 94.6% - 97.4% | 2.0% - 3.9% | 0.5% - 1.1% |
| 2025 | Q1-Q3 | 88.5% - 96.5% | 2.7% - 3.0% | 0.5% - 0.6% |

**Why boundary quarters have lower rates**: The first 4 and last 4 quarters of any consecutive quarterly dataset will always include (UPA, V1014) groups with fewer than 5 visits in the data. This makes mensalization efficiency suboptimal at the boundaries. In a dataset with 9 consecutive quarters, for example, only the 5th quarter will have full utilization -- because among people observed in the 5th quarter, all of them will have all their visits in the data, from those on their 1st visit in the 5th quarter to those on their 5th visit in the 5th quarter. This is an inherent consequence of PNADC's rotating panel design (5 visits over 5 quarters).

### Optimization Tips

- Use `data.table` directly (automatic conversion from `data.frame` adds overhead)
- Load only required columns when reading data
- Process multiple years together (cross-quarter aggregation improves accuracy)

---

## Tips and Best Practices

1. **Process multiple quarters together**: Processing 2012-2025 together gives ~97% determination vs ~73% for a single quarter.

2. **Start with reference month identification**: You don't always need monthly weights. Often just knowing the reference month is enough.

3. **Check determination rates by year**: Monthly rates should be 97-99% for 2013-2019. Lower rates may indicate data issues.

   ```{r check-rates}
   crosswalk[, .(rate = mean(!is.na(ref_month_in_quarter))), by = .(Ano, Trimestre)]
   ```

4. **Handle indeterminate observations**: Decide whether to exclude them or use quarterly-level analysis for those cases.

5. **Use `weight_monthly` for general analysis**: The rake-weighted output is appropriate for most purposes.

---

## Complete Workflow Example

This section demonstrates the complete workflow from raw PNADC data to monthly time series, incorporating all the concepts from this vignette.

### Scenario: Monthly Unemployment Analysis (2019-2024)

```{r complete-workflow}
library(PNADCperiods)
library(data.table)

# =============================================================================
# STEP 1: Load and stack quarterly PNADC data
# =============================================================================

# Load quarterly data (recommend 2+ years for good determination rates)
# Real data should be loaded from your files, e.g.:
# files <- list.files("path/to/data", pattern = "pnadc_.*\\.fst$", full.names = TRUE)
# pnadc_stacked <- rbindlist(lapply(files, fst::read_fst, as.data.table = TRUE))

# Required columns for identification:
required_id <- c("Ano", "Trimestre", "UPA", "V1008", "V1014",
                 "V2003", "V2008", "V20081", "V20082", "V2009")

# Additional columns for calibration:
required_weights <- c("V1028", "UF", "posest", "posest_sxi")

# Additional columns for analysis (labor force variables):
analysis_vars <- c("VD4001", "VD4002")  # Labor force status, occupation status

# =============================================================================
# STEP 2: Validate input data
# =============================================================================

validation <- validate_pnadc(pnadc_stacked, stop_on_error = FALSE)
print(validation)

# Check data coverage
pnadc_stacked[, .N, by = .(Ano, Trimestre)][order(Ano, Trimestre)]

# =============================================================================
# STEP 3: Build crosswalk (identify reference periods)
# =============================================================================

# This identifies months, fortnights, and weeks for all observations
crosswalk <- pnadc_identify_periods(pnadc_stacked, verbose = TRUE)

# Check determination rates by year
crosswalk[, .(
  n = .N,
  det_month = round(mean(determined_month, na.rm = TRUE) * 100, 1),
  det_fortnight = round(mean(determined_fortnight, na.rm = TRUE) * 100, 1),
  det_week = round(mean(determined_week, na.rm = TRUE) * 100, 1)
), by = Ano]

# Expected output:
#    Ano        n det_month det_fortnight det_week
# 1: 2019  2284532      97.2           4.1      1.3
# 2: 2020  2156789      96.1           3.8      1.1
# ...

# =============================================================================
# STEP 4: Apply crosswalk and calibrate monthly weights
# =============================================================================

result <- pnadc_apply_periods(
  pnadc_stacked,
  crosswalk,
  weight_var = "V1028",
  anchor = "quarter",
  calibration_unit = "month",
  calibrate = TRUE,
  keep_all = TRUE,  # Keep indeterminate observations (with NA weights)
  verbose = TRUE
)

# Verify population totals match SIDRA
result[!is.na(weight_monthly), .(
  pop_millions = sum(weight_monthly) / 1e6
), by = ref_month_yyyymm][order(ref_month_yyyymm)]

# =============================================================================
# STEP 5: Compute monthly estimates
# =============================================================================

# Calculate monthly unemployment rate
monthly_labor <- result[!is.na(ref_month_in_quarter), .(
  labor_force = sum(weight_monthly * (VD4001 == 1), na.rm = TRUE),
  employed = sum(weight_monthly * (VD4002 == 1), na.rm = TRUE),
  n_obs = .N
), by = .(ref_month_yyyymm, ref_month)]

monthly_labor[, unemployment_rate := (labor_force - employed) / labor_force * 100]

# Check sample sizes
summary(monthly_labor$n_obs)  # Should be ~65,000+ per month

# =============================================================================
# STEP 6: Visualize results
# =============================================================================

# Basic plot (requires ggplot2)
# library(ggplot2)
# ggplot(monthly_labor, aes(x = ref_month, y = unemployment_rate)) +
#   geom_line() +
#   geom_point(size = 1) +
#   labs(title = "Monthly Unemployment Rate - Brazil",
#        subtitle = "From PNADC quarterly data using PNADCperiods",
#        x = NULL, y = "Unemployment Rate (%)") +
#   theme_minimal()
```

### Key Takeaways

1. **Stack data first**: Load 2+ years before calling `pnadc_identify_periods()` to maximize determination rate.

2. **Validate inputs**: Use `validate_pnadc()` to catch missing columns early.

3. **Check determination rates**: Rates should be ~96-99% for stable years. Lower rates suggest data issues or boundary effects.

4. **Use `keep_all = TRUE`**: Preserves all observations; indeterminate ones get `NA` weights.

5. **Weight smoothing** (`smooth = TRUE` by default): Keep enabled for trend analysis; disable for shock/event analysis.

6. **Verify population totals**: Monthly weighted sums should approximate Brazil's population (~206 million in 2024).

---

## Further Reading

- [Applied Examples](applied-examples.html) - See the algorithm in action with COVID, recession, and minimum wage validation
- [Annual Poverty Analysis](annual-poverty-analysis.html) - Monthly poverty analysis with annual PNADC income data
- [IBGE PNADC Documentation](https://www.ibge.gov.br/estatisticas/sociais/trabalho/9171-pesquisa-nacional-por-amostra-de-domicilios-continua-mensal.html)
- Package function reference: `?pnadc_identify_periods`, `?pnadc_apply_periods`
- Source code: [GitHub repository](https://github.com/antrologos/mensalizePNADC)
